Goal
----

Quality objectives:
good: precision > 99%, recall > 50%
acceptable?: precision > 95%, recall >

Improvements
------------

Clean the full text
Use Bert
Address imbalance
Try TFIDF+KNN
Try NB
S Check secondary categories
Analyse ceiling
S lump all the under-represented classes together

DONE add cat titles to the training set
    not beneficial
DONE use autotune with validation set
    not real improvement but b/c sacrifice 1 sample?
WIP use the text content to classify
    not trivial to split it and align with titles
TODO try Flair library for classification
WONT augment training set from other sources using the same classification
    couldn't find such datasets
TODO use text content to create embeddings
TODO re-balance training set
    by duplicating samples
    CHECK by removing samples
        seems to correct biggest spill out mistakes and increases accuracy by 3-4%
remove acts/resolution
exploit hierarchical relationships

Conclusions
-----------

Analysis:
78% for level 1 cat, 61% for level 2, 54% for level 3.
A 3rd of the corpus can be classified with ~5% mistakes.
    10k articles => classify 3+k and 150 mistakes, some minors.
Can get a level of certainty but lots of false positive.
This method needs sufficient
? probably not enough data
? might be possible to use imperfect text but need some automated cleanup

Recommendations:
DONE More data (500-1500 more), especially for underrepresented classes
    Doesn't improve L1, but does with L2 & 3 (10% each)
DONE Would start with software to extract titles from OCRed pdfs (also split texts)
Some regexp (e.g. ref to others), simple heuristics could also help pre-process things
Semi-automated process with feedback loop would be more realistic

To explore:
DONE Use more training data
DONE Use article text for classification
    Worse performance (apart from L1)
Try different classifiers (e.g. recurrent net, Flair, Bert)

Questions
---------

DONE Q1a: how much prediction errors affected by under-representation of some classes?
    => return confusion matrix and compare with class distribution
    depth 1: yes, over-represented classes steal from under-represented ones (1, 2, 7 from 5, 3 & 6)
    see confusion matrix:
        acc/cl = diag/tot,
        prec = diag / bottom, (i.e. matching colors = excellent)
        rec = diag / right (i.e. diag is orange = excellent)
DONE Q1b: if there is a strong correlation, can we estimate how many sample/class are needed to get much better results?

DONE Q2: what is the cut-off confidence level? And what automation benefit can we expect from it?
    => return a table showing the rec+prec matrix for class vs confidence cut-off

26 dec
Q3. Which mistakes are the worst in L1?
    Best classes: 4, 0, 8
    Worst classes (titles): 2, 1 (stolen by 2, 8), 6 (stolen by 3, 4, 8)
    Worst classes (fulltx):    1 (stolen by 2, 3), 6 (stolen by 7, 4   ), 7
    Finance8 steals from Gov1, and Religion4 steals from Social6
    Under-represented: 5 (50 instead of 40)
    => no correlation b/w class size and error rate

0. personal, 1. gov, 2. finance, 3. law and order,
4. religion, 5. defence, 6. social, 7. economy, 8. comms

Reading
-------

https://www.ehs.org.uk/dotAsset/4fb9d095-5599-4885-af0d-e14211c4b490.pdf
see footnotes page 5

Prob1: unsupervised/emebbings model saved as .bin (all model params, thus ngrams)
but supervised can only take a .vec which contains word embeddings.

Note that FT avg ngram vecs to get a word vec

https://stackoverflow.com/questions/47118678/difference-between-fasttext-vec-and-bin-file
https://github.com/facebookresearch/fastText/issues/469

doc2vec
https://github.com/kazuki-hayakawa/fasttext_vs_doc2vec/blob/master/model_doc2vec/make_d2v_model.py
https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568
https://rare-technologies.com/sent2vec-an-unsupervised-approach-towards-learning-sentence-embeddings/
https://towardsdatascience.com/text-classification-with-extremely-small-datasets-333d322caee2

Trial logs
----------

Depth 3

386 training, 68 testing
Read 0M words
Number of words:  1223
Number of labels: 70
Progress: 100.0% words/sec/thread:  407111 lr:  0.000000 avg.loss:  0.029829 ETA:   0h 0m 0s
acc: 0.53 certain: 0.46 acc certain: 0.77 0.35
avg: 0.56 [0.51, 0.65], depth: 3, 10 trials, 100 dims, 200 epochs, (Embedddings: Law2Vec.100d.txt)

10 trials
100 dims
200 epochs
law2vec
Read 0M words
Number of words:  1234
Number of labels: 70
Progress: 100.0% words/sec/thread:  697935 lr:  0.000000 avg.loss:  0.025825 ETA:   0h 0m 0s
acc: 0.51 certain: 0.44 acc certain: 0.77 0.34
avg: 0.54 [0.49, 0.59]

500 epochs
acc: 0.51 certain: 0.41 acc certain: 0.93 0.38
avg: 0.55 [0.50, 0.60]

self
200 epochs
acc: 0.49 certain: 0.16 acc certain: 0.91 0.15
avg: 0.46 [0.37, 0.56]

500 epochs
acc: 0.47 certain: 0.25 acc certain: 0.76 0.19
avg: 0.52 [0.47, 0.57]

1 trial
200 epochs?
300 dims
wiki-news-300d-1M-subword.vec
Read 0M words
Number of words:  1250
Number of labels: 70
Progress: 100.0% words/sec/thread:  266484 lr:  0.000000 avg.loss:  0.010647 ETA:   0h 0m 0s
acc: 0.53 certain: 0.56 acc certain: 0.68 0.38
avg: 0.53 [0.53, 0.53]

--
Depth 1

10 trials
200 epochs
law2vec
Read 0M words
Number of words:  1319
Number of labels: 9
Progress: 100.0% words/sec/thread: 1937722 lr:  0.000000 avg.loss:  0.008231 ETA:   0h 0m 0s
acc: 0.69 certain: 0.56 acc certain: 0.78 0.44
avg: 0.84 [0.69, 1.00]

Law2Vec
200 epochs
10 trials
acc: 0.75 certain: 0.75 acc certain: 0.83 0.62
avg: 0.76 [0.62, 0.94]

self
acc: 0.81 certain: 0.62 acc certain: 0.90 0.56
avg: 0.74 [0.62, 0.94]

?
438 training, 16 testing
Read 0M words
Number of words:  1335
Number of labels: 9
acc: 0.75 certain: 0.75 acc certain: 0.92 0.69
avg: 0.75 [0.75, 0.75]

---
Depth 2

acc: 0.58 certain: 0.47 acc certain: 0.65 0.31
avg: 0.62 [0.53, 0.72], depth: 2, 10 trials, 100 dims, 500 epochs, (Embedddings: None)

