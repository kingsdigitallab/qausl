Goal
----

Quality objectives:
good: precision > 99%, recall > 50%
acceptable?: precision > 95%, recall >

Improvements
------------

M Other types of classifiers
    M Use Bert (L1 titles unless otherwise specified)
        DONE Flair TARS-0
            ++ no training :)
            - very poor accuracy (33% on L1)
            - quite slow to test each title
        Flair TARS-n
            ++ looks very promising on 1 random trial (~84% after 4 epochs on titles)
            - But heavy (0.5 GB model) and demanding/slow to build (~1min on GPU), load and test
            - can't train on full text, it blows up RAM
            - uses a validation set, which reduces available training & class recognition?
            => So Bert looks promising but we need a smaller architecture
    C Try TFIDF+KNN
    C Try NB
    D FastText
        ++ very good results and very fast compared to more advanced models, can run easily on laptop CPU
S Clean the full text
    DONE misassigned chapters
    DONE very short chapter text
    words stuck together
    other OCR misspellings
    stop words
C Address imbalance
    M lump all the under-represented classes together
    W Using another loss function
S Analyse errors and ceiling
    S F1 score
    C Show results as a html pages
C Optimise FastText:
    DONE try FT with ngram=2 words, it's supposed to improve results
        Tried on L1 & 2, doesn't seem to make any difference
    C lr=1.0 & epoch=25

DONE Accept secondary categories as a true positive
    slight increase (2%), nothing major.
DONE add cat titles to the training set
    not beneficial
DONE use autotune with validation set
    not real improvement but b/c sacrifice 1 sample?
WIP use the text content to classify
    not trivial to split it and align with titles
TODO try Flair library for classification
WONT augment training set from other sources using the same classification
    couldn't find such datasets
TODO use text content to create embeddings
TODO re-balance training set
    by duplicating samples
    CHECK by removing samples
        seems to correct biggest spill out mistakes and increases accuracy by 3-4%
remove acts/resolution
exploit hierarchical relationships

Legislation Subject scheme (dec 2020)
--------------------------

Julian Hoppit, 2017, Britain's Political Economies: Parliament and Economic Life, 1660â€“1800, p.328
https://books.google.co.uk/books?id=074pDwAAQBAJ&pg=PA328&lpg=PA328&dq=%22Loans+and+national+debt+management%22&source=bl&ots=7gj6fEgmnC&sig=ACfU3U3Xi_8yItKMUNXlY_73XT2uMLzEOg&hl=en&sa=X&ved=2ahUKEwjgrqeQhPPtAhWRYcAKHVnDALQQ6AEwBHoECAIQAg#v=onepage&q=%22Loans%20and%20national%20debt%20management%22&f=false

David Hayton, Eveline Cruickshanks, Stuart Handley, 2002, The House of Commons, 1690-1715, Volume 1, p. 937
https://books.google.co.uk/books?id=XdphWLO1xLsC&pg=RA3-PA938&lpg=RA3-PA938&dq=%22Loans+and+national+debt+management%22&source=bl&ots=M-bk1ZGKxc&sig=ACfU3U2_H1FfGr2Btt44lUvzVWUaPv6k_w&hl=en&sa=X&ved=2ahUKEwjgrqeQhPPtAhWRYcAKHVnDALQQ6AEwA3oECAEQAg#v=onepage&q=%22Loans%20and%20national%20debt%20management%22&f=false

Irish Legislation Database
https://www.qub.ac.uk/ild/?func=advanced_search&search=true&search_string=&search_string_type=ALL&search_type=any&session_from=1692&session_to=1800&enacted_search=all&subjects=%7C205%7C

Removed all Irish or English specifics from classes.txt, e.g. Bank of England

Conclusions
-----------

Analysis:
78% for level 1 cat, 61% for level 2, 54% for level 3.
A 3rd of the corpus can be classified with ~5% mistakes.
    10k articles => classify 3+k and 150 mistakes, some minors.
Can get a level of certainty but lots of false positive.
This method needs sufficient
? probably not enough data
? might be possible to use imperfect text but need some automated cleanup

Recommendations:
DONE More data (500-1500 more), especially for underrepresented classes
    Doesn't improve L1, but does with L2 & 3 (10% each)
DONE Would start with software to extract titles from OCRed pdfs (also split texts)
Some regexp (e.g. ref to others), simple heuristics could also help pre-process things
Semi-automated process with feedback loop would be more realistic

To explore:
DONE Use more training data
DONE Use article text for classification
    Worse performance (apart from L1)
Try different classifiers (e.g. recurrent net, Flair, Bert)

[Dec]

!!!! Reason we can't learn 100% level 3 from titles only
    208 Relief of James Ralston
    007 Relief of Andres Patterson
    001 Relief of Marcus Hulings, junior
    000 Relief of John Hughes
    006 Relief of Robert Cunningham, a prisoner in the gaol of the county of Philadelphia
    009 Relief of Thomas Butler and Henry Brown

Questions
---------

DONE Q1a: how much prediction errors affected by under-representation of some classes?
    => return confusion matrix and compare with class distribution
    depth 1: yes, over-represented classes steal from under-represented ones (1, 2, 7 from 5, 3 & 6)
    see confusion matrix:
        acc/cl = diag/tot,
        prec = diag / bottom, (i.e. matching colors = excellent)
        rec = diag / right (i.e. diag is orange = excellent)
DONE Q1b: if there is a strong correlation, can we estimate how many sample/class are needed to get much better results?

DONE Q2: what is the cut-off confidence level? And what automation benefit can we expect from it?
    => return a table showing the rec+prec matrix for class vs confidence cut-off

26 dec
Q3. Which mistakes are the worst in L1?
    Best classes: 4, 0, 8
    Worst classes (titles): 2, 1 (stolen by 2, 8), 6 (stolen by 3, 4, 8)
    Worst classes (fulltx):    1 (stolen by 2, 3), 6 (stolen by 7, 4   ), 7
    Finance8 steals from Gov1, and Religion4 steals from Social6
    Under-represented: 5 (50 instead of 40)
    => no correlation b/w class size and error rate

0. personal, 1. gov, 2. finance, 3. law and order,
4. religion, 5. defence, 6. social, 7. economy, 8. comms

Reading
-------

https://www.ehs.org.uk/dotAsset/4fb9d095-5599-4885-af0d-e14211c4b490.pdf
see footnotes page 5

Prob1: unsupervised/emebbings model saved as .bin (all model params, thus ngrams)
but supervised can only take a .vec which contains word embeddings.

Note that FT avg ngram vecs to get a word vec

https://stackoverflow.com/questions/47118678/difference-between-fasttext-vec-and-bin-file
https://github.com/facebookresearch/fastText/issues/469

doc2vec
https://github.com/kazuki-hayakawa/fasttext_vs_doc2vec/blob/master/model_doc2vec/make_d2v_model.py
https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568
https://rare-technologies.com/sent2vec-an-unsupervised-approach-towards-learning-sentence-embeddings/
https://towardsdatascience.com/text-classification-with-extremely-small-datasets-333d322caee2

Trial logs
----------

Depth 3

386 training, 68 testing
Read 0M words
Number of words:  1223
Number of labels: 70
Progress: 100.0% words/sec/thread:  407111 lr:  0.000000 avg.loss:  0.029829 ETA:   0h 0m 0s
acc: 0.53 certain: 0.46 acc certain: 0.77 0.35
avg: 0.56 [0.51, 0.65], depth: 3, 10 trials, 100 dims, 200 epochs, (Embedddings: Law2Vec.100d.txt)

10 trials
100 dims
200 epochs
law2vec
Read 0M words
Number of words:  1234
Number of labels: 70
Progress: 100.0% words/sec/thread:  697935 lr:  0.000000 avg.loss:  0.025825 ETA:   0h 0m 0s
acc: 0.51 certain: 0.44 acc certain: 0.77 0.34
avg: 0.54 [0.49, 0.59]

500 epochs
acc: 0.51 certain: 0.41 acc certain: 0.93 0.38
avg: 0.55 [0.50, 0.60]

self
200 epochs
acc: 0.49 certain: 0.16 acc certain: 0.91 0.15
avg: 0.46 [0.37, 0.56]

500 epochs
acc: 0.47 certain: 0.25 acc certain: 0.76 0.19
avg: 0.52 [0.47, 0.57]

1 trial
200 epochs?
300 dims
wiki-news-300d-1M-subword.vec
Read 0M words
Number of words:  1250
Number of labels: 70
Progress: 100.0% words/sec/thread:  266484 lr:  0.000000 avg.loss:  0.010647 ETA:   0h 0m 0s
acc: 0.53 certain: 0.56 acc certain: 0.68 0.38
avg: 0.53 [0.53, 0.53]

--
Depth 1

10 trials
200 epochs
law2vec
Read 0M words
Number of words:  1319
Number of labels: 9
Progress: 100.0% words/sec/thread: 1937722 lr:  0.000000 avg.loss:  0.008231 ETA:   0h 0m 0s
acc: 0.69 certain: 0.56 acc certain: 0.78 0.44
avg: 0.84 [0.69, 1.00]

Law2Vec
200 epochs
10 trials
acc: 0.75 certain: 0.75 acc certain: 0.83 0.62
avg: 0.76 [0.62, 0.94]

self
acc: 0.81 certain: 0.62 acc certain: 0.90 0.56
avg: 0.74 [0.62, 0.94]

?
438 training, 16 testing
Read 0M words
Number of words:  1335
Number of labels: 9
acc: 0.75 certain: 0.75 acc certain: 0.92 0.69
avg: 0.75 [0.75, 0.75]

---
Depth 2

acc: 0.58 certain: 0.47 acc certain: 0.65 0.31
avg: 0.62 [0.53, 0.72], depth: 2, 10 trials, 100 dims, 500 epochs, (Embedddings: None)


--
